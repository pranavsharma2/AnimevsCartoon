{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sz=224\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = 'datanew/anime-cartoon/train'\n",
    "validation_data_dir = 'datanew/anime-cartoon/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 208 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "    shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "    target_size=(sz, sz),\n",
    "    batch_size=batch_size, class_mode='binary')\n",
    "label_map = (train_generator.class_indices)\n",
    "validation_generator = test_datagen.flow_from_directory(validation_data_dir,\n",
    "    shuffle=False,\n",
    "    target_size=(sz, sz),\n",
    "    batch_size=batch_size, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "for layer in base_model.layers: layer.trainable = False\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/PIL/Image.py:870: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 68s 1s/step - loss: 2.1943 - acc: 0.7308 - val_loss: 1.4508 - val_acc: 0.7727\n",
      "Epoch 2/2\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.8991 - acc: 0.8558 - val_loss: 0.5487 - val_acc: 0.8864\n",
      "CPU times: user 7min 27s, sys: 9.14 s, total: 7min 36s\n",
      "Wall time: 2min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f063ab7ee48>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit_generator(train_generator, train_generator.n // batch_size, epochs=2, workers=4,\n",
    "        validation_data=validation_generator, validation_steps=validation_generator.n // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_at = 140\n",
    "for layer in model.layers[:split_at]: layer.trainable = False\n",
    "for layer in model.layers[split_at:]: layer.trainable = True\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "11/52 [=====>........................] - ETA: 1:03 - loss: 0.2602 - acc: 0.8864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/PIL/Image.py:870: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 88s 2s/step - loss: 0.2443 - acc: 0.9279 - val_loss: 4.6662 - val_acc: 0.6364\n",
      "Epoch 2/2\n",
      "52/52 [==============================] - 85s 2s/step - loss: 0.1095 - acc: 0.9615 - val_loss: 0.7005 - val_acc: 0.8636\n",
      "CPU times: user 9min 49s, sys: 17.1 s, total: 10min 6s\n",
      "Wall time: 2min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f05f4689710>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit_generator(train_generator, train_generator.n // batch_size, epochs=2, workers=4,\n",
    "        validation_data=validation_generator, validation_steps=validation_generator.n // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anime': 0, 'cartoon': 1}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
